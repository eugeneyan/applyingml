{
    "componentChunkName": "component---src-templates-post-js",
    "path": "/resources/rl-for-recsys/",
    "result": {"data":{"mdx":{"frontmatter":{"title":"Reinforcement Learning for Recommendation Systems","description":"","name":null,"role":null,"slug":"/resources/rl-for-recsys/","type":"resource"},"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Reinforcement Learning for Recommendation Systems\",\n  \"description\": \"\",\n  \"date\": \"2021-09-05\",\n  \"slug\": \"/resources/rl-for-recsys/\",\n  \"type\": \"resource\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Most recommendation systems learn user preferences and item popularity from historical data, to retrain models at periodic intervals. This comes with a few downsides.\"), mdx(\"p\", null, \"First, they\\u2019re designed to maximize the immediate reward of making users click or purchase, and don\\u2019t consider long-term rewards such as user activeness. Furthermore, they tend to adopt a greedy approach and overemphasize item popularity, neglecting to explore new items (i.e., cold-start problem). Finally, they don\\u2019t work well when new items are frequently added (e.g., news articles, TikTok/YouTube, etc).\"), mdx(\"p\", null, \"One way to address this is via reinforcement learning. RL can learn to optimize for long-term rewards, balance exploration and exploitation, and continuously learn online. Here, we explore various reinforcement learning approaches for recommendation systems, including bandits, value-based methods, and policy-based methods.\"), mdx(\"h2\", {\n    \"id\": \"contextual-bandits-multi-armed-bandits-with-context\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Contextual bandits: Multi-armed bandits with context\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#contextual-bandits-multi-armed-bandits-with-context\",\n    \"aria-label\": \"contextual bandits multi armed bandits with context permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"height\": \"20px\",\n    \"width\": \"20px\",\n    \"style\": {\n      \"verticalAlign\": \"-10%\",\n      \"marginLeft\": \"0.25rem\"\n    },\n    \"fill\": \"#007bff\",\n    \"x\": \"0px\",\n    \"y\": \"0px\",\n    \"viewBox\": \"0 0 100 100\",\n    \"enableBackground\": \"new 0 0 100 52\",\n    \"xmlSpace\": \"preserve\"\n  }, mdx(\"g\", {\n    parentName: \"svg\",\n    \"transform\": \"matrix(0.6136733,-0.6136733,0.6136733,0.6136733,4.0598725,65.6509)\"\n  }, mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.807,33.138 c 0.008,-0.009 0.016,-0.019 0.024,-0.026 -0.009,0.007 -0.018,0.015 -0.026,0.023 0,0.001 10e-4,0.002 0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 53.048,49.8 c -0.274,-0.071 -0.545,-0.152 -0.815,-0.232 -0.002,0.001 -0.005,0.003 -0.007,0.003 0.259,0.077 0.534,0.155 0.822,0.229 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 36.33,35.501 h -0.004 c 0.204,0.522 0.496,1.114 0.783,1.651 h 0.013 c -0.284,-0.54 -0.548,-1.09 -0.792,-1.651 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.108,37.152 c 0.291,0.544 0.577,1.029 0.758,1.332 -0.263,-0.436 -0.508,-0.882 -0.746,-1.332 h -0.012 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 38.062,38.804 h 0.006 c -0.048,-0.079 -0.1,-0.152 -0.147,-0.231 0.087,0.144 0.141,0.231 0.141,0.231 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c -0.631,-0.197 -1.249,-0.424 -1.856,-0.669 -0.003,0.002 -0.007,0.003 -0.009,0.004 0.06,0.025 0.781,0.324 1.865,0.665 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 52.226,49.57 c 0.002,0 0.005,-0.002 0.007,-0.003 -0.106,-0.03 -0.213,-0.057 -0.318,-0.091 0.101,0.033 0.205,0.065 0.311,0.094 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 74.458,0.955 H 59.321 c -1.307,0 -2.591,0.1 -3.845,0.293 -0.273,0.042 -0.541,0.101 -0.811,0.151 4.646,2.666 8.469,6.605 10.987,11.343 h 1.238 8.346 c 7.322,0 13.257,5.937 13.257,13.259 0,7.321 -5.936,13.258 -13.257,13.258 H 65.72 63.762 61.857 61.851 58.68 c -3.305,0 -6.324,-1.212 -8.646,-3.212 0,0 -0.003,0.003 -0.003,0.003 0,-0.002 -10e-4,-0.002 -0.002,-0.003 -0.494,-0.414 -0.886,-0.802 -1.206,-1.148 -0.7,-0.764 -1.011,-1.303 -1.011,-1.303 0.001,-0.002 0.002,-0.002 0.002,-0.003 -10e-4,-0.001 -0.002,-0.002 -0.002,-0.003 -1.505,-2.15 -2.389,-4.766 -2.389,-7.589 0,-2.746 0.834,-5.296 2.263,-7.413 0.041,-0.06 0.086,-0.117 0.127,-0.176 -1.736,-1.473 -3.978,-2.367 -6.429,-2.367 h -5.041 c -1.325,3.052 -2.067,6.416 -2.067,9.956 0,3.538 0.737,6.903 2.061,9.955 0.244,0.561 0.509,1.111 0.792,1.651 0.237,0.45 0.482,0.896 0.746,1.332 0.02,0.033 0.036,0.059 0.054,0.088 0.047,0.079 0.099,0.152 0.147,0.231 2.792,4.463 6.948,7.98 11.888,9.973 0.608,0.245 1.226,0.472 1.856,0.669 0.034,0.011 0.068,0.022 0.103,0.031 0.105,0.034 0.212,0.061 0.318,0.091 0.27,0.08 0.541,0.161 0.815,0.232 0.004,0 0.006,0.002 0.01,0.003 2,0.513 4.096,0.787 6.254,0.787 H 74.457 C 88.289,51.044 99.501,39.831 99.501,26 99.502,12.169 88.29,0.955 74.458,0.955 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 25.542,51.045 h 15.137 c 1.308,0 2.592,-0.101 3.846,-0.294 0.273,-0.042 0.542,-0.1 0.811,-0.151 C 40.69,47.935 36.866,43.996 34.349,39.258 H 33.11 24.764 c -7.322,0 -13.257,-5.936 -13.257,-13.259 0,-7.322 5.935,-13.257 13.257,-13.257 h 9.516 1.957 1.905 0.007 3.17 c 3.304,0 6.323,1.212 8.645,3.212 l 0.002,-0.003 c 10e-4,10e-4 10e-4,0.002 0.002,0.003 0.495,0.413 0.887,0.801 1.206,1.148 0.701,0.762 1.011,1.302 1.011,1.302 0,0.001 -0.001,0.002 -0.002,0.003 10e-4,0 0.002,0.001 0.003,0.003 1.505,2.15 2.39,4.766 2.39,7.588 0,2.746 -0.835,5.297 -2.264,7.412 -0.041,0.062 -0.085,0.118 -0.127,0.176 1.736,1.474 3.979,2.369 6.429,2.369 h 5.042 c 1.324,-3.053 2.065,-6.417 2.065,-9.957 0,-3.539 -0.737,-6.903 -2.061,-9.955 C 63.416,15.482 63.151,14.932 62.868,14.392 62.631,13.941 62.386,13.494 62.122,13.06 62.102,13.027 62.086,13 62.069,12.971 62.021,12.893 61.97,12.818 61.922,12.741 59.13,8.278 54.973,4.76 50.034,2.768 49.426,2.523 48.808,2.297 48.178,2.098 48.144,2.087 48.11,2.077 48.075,2.066 47.97,2.034 47.863,2.007 47.757,1.977 47.487,1.896 47.216,1.815 46.941,1.745 46.937,1.744 46.935,1.744 46.932,1.743 44.932,1.228 42.837,0.955 40.677,0.955 H 25.542 c -13.832,0 -25.044,11.212 -25.044,25.044 0,13.833 11.212,25.046 25.044,25.046 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 53.058,49.803 C 53.055,49.802 53.052,49.8 53.048,49.8 c 0.004,0 0.007,0.002 0.01,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c 0.034,0.011 0.068,0.022 0.103,0.031 -0.034,-0.008 -0.068,-0.02 -0.103,-0.031 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.805,33.141 c 0,0 0.311,0.539 1.011,1.303 0.005,-0.006 0.01,-0.012 0.016,-0.015 -0.369,-0.407 -0.709,-0.84 -1.025,-1.291 0,0.001 -0.001,0.001 -0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.92,38.572 c -0.018,-0.029 -0.034,-0.055 -0.054,-0.088 0.019,0.03 0.036,0.058 0.054,0.088 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 48.832,34.429 c -0.006,0.003 -0.011,0.009 -0.016,0.015 0.319,0.347 0.711,0.734 1.206,1.148 0.001,-0.002 0.002,-0.003 0.002,-0.003 -0.42,-0.362 -0.82,-0.749 -1.192,-1.16 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 50.027,35.592 c 10e-4,-0.003 0.004,-0.006 0.012,-0.015 -0.004,0.004 -0.01,0.008 -0.015,0.012 10e-4,0 0.002,0.001 0.003,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"rect\", {\n    parentName: \"g\",\n    \"height\": \"0.004007bff2\",\n    \"width\": \"0.004007bff2\",\n    \"y\": \"35.59\",\n    \"x\": \"50.021999\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }))))), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Multi-armed_bandit\"\n  }, \"Multi-armed bandits\"), \" are a form of classical reinforcement learning. They try to balance exploration and exploitation by exploring new actions to learn what the potential reward is, and then exploiting the current best action to maximize reward. The goal is to learn about and choose actions that maximize total reward (aka minimize regret).\"), mdx(\"p\", null, \"Contextual bandits take it a step further\\u2014they collect and observe the context before each action, and choose actions based on the context. They learn about how actions \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"and context\"), \" affect reward. In the case of recommendations and search, the context would be data we have about the customer (e.g., demographics, device, indicated/historical preferences) and environment (e.g., day of week, time of day).\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Yahoo shared about using a \", mdx(\"a\", {\n    parentName: \"strong\",\n    \"href\": \"https://arxiv.org/abs/1003.0146\"\n  }, \"contextual bandit for recommending news articles\"), \".\"), \" While regular bandits only learn on features specific to each arm (i.e., disjoint linear models), their approach uses features\\u2014such as user-article cross features\\u2014that are shared across all arms (i.e., hybrid linear models). Experiments showed that this allowed click-through rate information from one article to be learned and applied to other articles (i.e., transfer learning)\\u2014this was not possible with disjoint linear models. \"), mdx(\"p\", null, \"For features, users were represented by 1,193 categorical features including demographics (age, gender), geography (locations, US states), and behavior (binary categories that summarize news consumption history). News articles were represented by 83 categorical features such as URL category (inferred from URL) and editor categories (manually tagged by human editors).\"), mdx(\"p\", null, \"To reduce dimensionality, they projected user features into article categories and then clustered users with similar preferences. The same technique was also applied to news article features. This resulted in user and article features that were six dimensions each. To create user-article cross features, they computed the outer product of user and article features, leading to a 36-dimension vector\\u2014this puts the hybrid in hybrid linear models.\"), mdx(\"p\", null, \"Evaluation was tricky as they only had offline data that was collected via a different policy (i.e., another recommender was in production when the data was collected, aka off-policy evaluation). Thus, they assumed that individual events were independent and identically distributed, and that the policy used to gather the logged data chose each arm uniformly at random. (They had a \\u201Clearning bucket\\u201D to which some users were randomly assigned to. Users in the learning bucket were served articles randomly.)\"), mdx(\"p\", null, \"Their policy evaluator takes in the learned policy and logged policy. If the learned policy chose the same arm as the logged policy, the event is retained and added to the history, and the payoff is updated. If the learned policy selects a different arm, then the event is ignored and the algorithm proceeds to the next event without any change in state. \"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Netflix also used \", mdx(\"a\", {\n    parentName: \"strong\",\n    \"href\": \"https://netflixtechblog.com/artwork-personalization-c589f074ad76\"\n  }, \"contextual bandits to personalize movie images\"), \" on the home page.\"), \" Why contextual bandits? They explained that batch machine learning approaches require time to collect data, train the model, and AB test, during which members do not benefit from the better experience (i.e., regret). To reduce regret, they moved towards online machine learning and adopted contextual bandits to continuously explore and learn about the best recommendation for each customer.\"), mdx(\"p\", null, \"They started with a (non-contextual) \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://netflixtechblog.com/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6\"\n  }, \"multi-armed bandit\"), \" to find the single best artwork for all users. Then, it progressed to contextual bandits to personalize images for each user. The bandit can choose from a set of images for each show (i.e., action) and observe the number of minutes the user played the show after being impressed with the image (i.e., reward). It also has information about user attributes (e.g., titles played, genres played, country, language preferences), day of week, time of day, etc. (i.e., context).\"), mdx(\"p\", null, \"They shared several bandit models, such as a greedy policy (via supervised regression model), epsilon greedy, LinUCB, and Thompson Sampling but weren't specific about which was used in production (probably an ensemble or something that's updated frequently). The evaluation approach was similar to Yahoo\\u2019s\\u2014I\\u2019ve written about it in more detail \", \"[here]\", \"({{ site.baseurl }}{% link posts/_posts/2021-06-13-patterns-for-personalization.md %}#bandits-learning-continuously-via-exploration).\"), mdx(\"h2\", {\n    \"id\": \"value-based-learning-value-for-each-state-action-pair\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Value-based: Learning value for each state-action pair\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#value-based-learning-value-for-each-state-action-pair\",\n    \"aria-label\": \"value based learning value for each state action pair permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"height\": \"20px\",\n    \"width\": \"20px\",\n    \"style\": {\n      \"verticalAlign\": \"-10%\",\n      \"marginLeft\": \"0.25rem\"\n    },\n    \"fill\": \"#007bff\",\n    \"x\": \"0px\",\n    \"y\": \"0px\",\n    \"viewBox\": \"0 0 100 100\",\n    \"enableBackground\": \"new 0 0 100 52\",\n    \"xmlSpace\": \"preserve\"\n  }, mdx(\"g\", {\n    parentName: \"svg\",\n    \"transform\": \"matrix(0.6136733,-0.6136733,0.6136733,0.6136733,4.0598725,65.6509)\"\n  }, mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.807,33.138 c 0.008,-0.009 0.016,-0.019 0.024,-0.026 -0.009,0.007 -0.018,0.015 -0.026,0.023 0,0.001 10e-4,0.002 0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 53.048,49.8 c -0.274,-0.071 -0.545,-0.152 -0.815,-0.232 -0.002,0.001 -0.005,0.003 -0.007,0.003 0.259,0.077 0.534,0.155 0.822,0.229 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 36.33,35.501 h -0.004 c 0.204,0.522 0.496,1.114 0.783,1.651 h 0.013 c -0.284,-0.54 -0.548,-1.09 -0.792,-1.651 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.108,37.152 c 0.291,0.544 0.577,1.029 0.758,1.332 -0.263,-0.436 -0.508,-0.882 -0.746,-1.332 h -0.012 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 38.062,38.804 h 0.006 c -0.048,-0.079 -0.1,-0.152 -0.147,-0.231 0.087,0.144 0.141,0.231 0.141,0.231 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c -0.631,-0.197 -1.249,-0.424 -1.856,-0.669 -0.003,0.002 -0.007,0.003 -0.009,0.004 0.06,0.025 0.781,0.324 1.865,0.665 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 52.226,49.57 c 0.002,0 0.005,-0.002 0.007,-0.003 -0.106,-0.03 -0.213,-0.057 -0.318,-0.091 0.101,0.033 0.205,0.065 0.311,0.094 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 74.458,0.955 H 59.321 c -1.307,0 -2.591,0.1 -3.845,0.293 -0.273,0.042 -0.541,0.101 -0.811,0.151 4.646,2.666 8.469,6.605 10.987,11.343 h 1.238 8.346 c 7.322,0 13.257,5.937 13.257,13.259 0,7.321 -5.936,13.258 -13.257,13.258 H 65.72 63.762 61.857 61.851 58.68 c -3.305,0 -6.324,-1.212 -8.646,-3.212 0,0 -0.003,0.003 -0.003,0.003 0,-0.002 -10e-4,-0.002 -0.002,-0.003 -0.494,-0.414 -0.886,-0.802 -1.206,-1.148 -0.7,-0.764 -1.011,-1.303 -1.011,-1.303 0.001,-0.002 0.002,-0.002 0.002,-0.003 -10e-4,-0.001 -0.002,-0.002 -0.002,-0.003 -1.505,-2.15 -2.389,-4.766 -2.389,-7.589 0,-2.746 0.834,-5.296 2.263,-7.413 0.041,-0.06 0.086,-0.117 0.127,-0.176 -1.736,-1.473 -3.978,-2.367 -6.429,-2.367 h -5.041 c -1.325,3.052 -2.067,6.416 -2.067,9.956 0,3.538 0.737,6.903 2.061,9.955 0.244,0.561 0.509,1.111 0.792,1.651 0.237,0.45 0.482,0.896 0.746,1.332 0.02,0.033 0.036,0.059 0.054,0.088 0.047,0.079 0.099,0.152 0.147,0.231 2.792,4.463 6.948,7.98 11.888,9.973 0.608,0.245 1.226,0.472 1.856,0.669 0.034,0.011 0.068,0.022 0.103,0.031 0.105,0.034 0.212,0.061 0.318,0.091 0.27,0.08 0.541,0.161 0.815,0.232 0.004,0 0.006,0.002 0.01,0.003 2,0.513 4.096,0.787 6.254,0.787 H 74.457 C 88.289,51.044 99.501,39.831 99.501,26 99.502,12.169 88.29,0.955 74.458,0.955 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 25.542,51.045 h 15.137 c 1.308,0 2.592,-0.101 3.846,-0.294 0.273,-0.042 0.542,-0.1 0.811,-0.151 C 40.69,47.935 36.866,43.996 34.349,39.258 H 33.11 24.764 c -7.322,0 -13.257,-5.936 -13.257,-13.259 0,-7.322 5.935,-13.257 13.257,-13.257 h 9.516 1.957 1.905 0.007 3.17 c 3.304,0 6.323,1.212 8.645,3.212 l 0.002,-0.003 c 10e-4,10e-4 10e-4,0.002 0.002,0.003 0.495,0.413 0.887,0.801 1.206,1.148 0.701,0.762 1.011,1.302 1.011,1.302 0,0.001 -0.001,0.002 -0.002,0.003 10e-4,0 0.002,0.001 0.003,0.003 1.505,2.15 2.39,4.766 2.39,7.588 0,2.746 -0.835,5.297 -2.264,7.412 -0.041,0.062 -0.085,0.118 -0.127,0.176 1.736,1.474 3.979,2.369 6.429,2.369 h 5.042 c 1.324,-3.053 2.065,-6.417 2.065,-9.957 0,-3.539 -0.737,-6.903 -2.061,-9.955 C 63.416,15.482 63.151,14.932 62.868,14.392 62.631,13.941 62.386,13.494 62.122,13.06 62.102,13.027 62.086,13 62.069,12.971 62.021,12.893 61.97,12.818 61.922,12.741 59.13,8.278 54.973,4.76 50.034,2.768 49.426,2.523 48.808,2.297 48.178,2.098 48.144,2.087 48.11,2.077 48.075,2.066 47.97,2.034 47.863,2.007 47.757,1.977 47.487,1.896 47.216,1.815 46.941,1.745 46.937,1.744 46.935,1.744 46.932,1.743 44.932,1.228 42.837,0.955 40.677,0.955 H 25.542 c -13.832,0 -25.044,11.212 -25.044,25.044 0,13.833 11.212,25.046 25.044,25.046 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 53.058,49.803 C 53.055,49.802 53.052,49.8 53.048,49.8 c 0.004,0 0.007,0.002 0.01,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c 0.034,0.011 0.068,0.022 0.103,0.031 -0.034,-0.008 -0.068,-0.02 -0.103,-0.031 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.805,33.141 c 0,0 0.311,0.539 1.011,1.303 0.005,-0.006 0.01,-0.012 0.016,-0.015 -0.369,-0.407 -0.709,-0.84 -1.025,-1.291 0,0.001 -0.001,0.001 -0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.92,38.572 c -0.018,-0.029 -0.034,-0.055 -0.054,-0.088 0.019,0.03 0.036,0.058 0.054,0.088 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 48.832,34.429 c -0.006,0.003 -0.011,0.009 -0.016,0.015 0.319,0.347 0.711,0.734 1.206,1.148 0.001,-0.002 0.002,-0.003 0.002,-0.003 -0.42,-0.362 -0.82,-0.749 -1.192,-1.16 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 50.027,35.592 c 10e-4,-0.003 0.004,-0.006 0.012,-0.015 -0.004,0.004 -0.01,0.008 -0.015,0.012 10e-4,0 0.002,0.001 0.003,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"rect\", {\n    parentName: \"g\",\n    \"height\": \"0.004007bff2\",\n    \"width\": \"0.004007bff2\",\n    \"y\": \"35.59\",\n    \"x\": \"50.021999\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }))))), mdx(\"p\", null, \"In reinforcement learning, value-based methods learn the optimal value function. This is either a state function (mapping the state to a value) or a state-action function (mapping the state-action to a value). Using the value function, the agent acts by choosing the action that has the highest value in each state\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"JD shared about using \", mdx(\"a\", {\n    parentName: \"strong\",\n    \"href\": \"https://arxiv.org/abs/1802.06501\"\n  }, \"deep Q-networks (DQN) and negative feedback for e-commerce recommendations\"), \".\"), \"  They shared that items that users skip (i.e., not click) provide useful signal about user preferences and should also be incorporated into recommender systems\\u2014a system with only positive items will not change its state or update its strategy when users skip the recommended items. \"), mdx(\"p\", null, \"Thus, they defined the state of their Markov decision process (MDP) to include negative items. Their MDP has two separate states: \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"State+\"), \" (previous N items that users clicked or purchased) and \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"State-\"), \" (previous N items that users skipped). For the transition, if users click/purchase the recommended item, \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"State+\"), \" is updated by not \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"State-\"), \"; if users skip the item, \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"State-\"), \" is updated but not \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"State+\"), \".\"), mdx(\"p\", null, \"As input for the DQN, gated recurrent units (GRUs) encode the sequence of positive and negative events into \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"S+\"), \" and \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"S-\"), \" embeddings. These embeddings are then concatenated with the candidate item (\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"a\"), \"). The first few hidden layers (for \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"S+\"), \" and \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"S-\"), \") are separated, the intuition is that we want to recommend items similar to the clicked/purchased items, but different from the skipped items. Thus, separating the first few hidden layers helps with capturing the distinct contributions of the positive and negative feedback.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"700px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"93.14285714285714%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAGAABAQADAAAAAAAAAAAAAAAAAAECAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB7+KRsRUBQf/EABcQAQADAAAAAAAAAAAAAAAAAAEAECD/2gAIAQEAAQUChTr/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAUEAEAAAAAAAAAAAAAAAAAAAAw/9oACAEBAAY/Ah//xAAbEAACAgMBAAAAAAAAAAAAAAAAARARITFBUf/aAAgBAQABPyE7zDePRd1TQnaOwtH/2gAMAwEAAgADAAAAEMgIAP/EABURAQEAAAAAAAAAAAAAAAAAAAEQ/9oACAEDAQE/ECE//8QAFhEBAQEAAAAAAAAAAAAAAAAAARAR/9oACAECAQE/EHYz/8QAHhAAAQMEAwAAAAAAAAAAAAAAAQARITFBUWEQcZH/2gAIAQEAAT8QULkxoY4eLAF2l0I5a6AAggjSaVfUAA1aZQMn/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"title\",\n    \"title\": \"title\",\n    \"src\": \"/static/193f7aaba86bdc84eed3a3bde5584d26/29d31/jd-rl-model.jpg\",\n    \"srcSet\": [\"/static/193f7aaba86bdc84eed3a3bde5584d26/e52aa/jd-rl-model.jpg 175w\", \"/static/193f7aaba86bdc84eed3a3bde5584d26/70ebb/jd-rl-model.jpg 350w\", \"/static/193f7aaba86bdc84eed3a3bde5584d26/29d31/jd-rl-model.jpg 700w\", \"/static/193f7aaba86bdc84eed3a3bde5584d26/4b190/jd-rl-model.jpg 800w\"],\n    \"sizes\": \"(max-width: 700px) 100vw, 700px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(FigureCaption, {\n    caption: \"JD's DQN to recommend items based on positive and negative events\",\n    source: \"https://arxiv.org/abs/1802.06501\",\n    mdxType: \"FigureCaption\"\n  }), mdx(\"p\", null, \"To reduce computation cost, they have a prior candidate retrieval step that reduces the search space. Candidates are selected by finding the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"N\"), \" most similar items for the past \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"M\"), \" items that users clicked and purchased. These candidates are then passed into the DQN as potential actions for ranking. \"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Microsoft also adopted a \", mdx(\"a\", {\n    parentName: \"strong\",\n    \"href\": \"https://dl.acm.org/doi/pdf/10.1145/3178876.3185994\"\n  }, \"DQN for news recommendations\"), \".\"), \" For features, they had:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"News features: 417 one-hot features such as provider, ranking, entity name, category, topic category, and historical click counts\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"User features: Attributes of features that users clicked on in the last 1 hour, 6 hours, 24 hours, 1 week, and 1 year, totaling 413 x 5 = 2065 features\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"User-news features: 25 features that describe the interaction between the user and specific news (e.g., frequency of entity, category, topic, provider) \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Context features: 32 features such as time, day, freshness of news (time between recommendation and news published)\")), mdx(\"p\", null, \"The DQN distinguishes between the four types of features. User and context features are considered \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"state\"), \" features while all features are considered \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"action\"), \" features. The intuition is that the reward for taking an action is related to all features, while the reward relevant to user attributes is impacted by user and context features only. Thus, the Q function is divided into value function (state features only) and advantage function (all features).\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"700px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"63.42857142857142%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe7KNA//xAAXEAADAQAAAAAAAAAAAAAAAAAAAREh/9oACAEBAAEFAm9sZCaf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhAAAwAAAAAAAAAAAAAAAAAAABEg/9oACAEBAAY/AhT/AP/EABsQAAIDAAMAAAAAAAAAAAAAAAABEUFREGHB/9oACAEBAAE/IWJVo4OuLOZddFvgj//aAAwDAQACAAMAAAAQEw//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAcEAEAAgEFAAAAAAAAAAAAAAABACEREFFxgbH/2gAIAQEAAT8QAZUGMBy0UZLmUh0iUUo2ewIW5n//2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"title\",\n    \"title\": \"title\",\n    \"src\": \"/static/d2b09a9cf2c83757a8b73f2aba8c532b/29d31/ms-rl-model.jpg\",\n    \"srcSet\": [\"/static/d2b09a9cf2c83757a8b73f2aba8c532b/e52aa/ms-rl-model.jpg 175w\", \"/static/d2b09a9cf2c83757a8b73f2aba8c532b/70ebb/ms-rl-model.jpg 350w\", \"/static/d2b09a9cf2c83757a8b73f2aba8c532b/29d31/ms-rl-model.jpg 700w\", \"/static/d2b09a9cf2c83757a8b73f2aba8c532b/4b190/ms-rl-model.jpg 800w\"],\n    \"sizes\": \"(max-width: 700px) 100vw, 700px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(FigureCaption, {\n    caption: \"Microsofts's DQN for news recommendations\",\n    source: \"https://dl.acm.org/doi/pdf/10.1145/3178876.3185994\",\n    mdxType: \"FigureCaption\"\n  }), mdx(\"p\", null, \"For reward, they combined (immediate) click-based reward and (long-term) user activeness reward. They reasoned that click metrics are only part of user feedback, and that recommendation systems should also consider whether the user will return to the application. User activeness was modeled via survival models and combined with click reward to get total reward. Nonetheless, user activeness reward had a low weight of 0.05.\"), mdx(\"p\", null, \"For exploration, they applied dueling bandit gradient descent. Two networks are used to create recommendations: the current network \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\"), \" and the exploration network \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\\u2019\"), \". Weights of \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\\u2019\"), \" are obtained by adding some noise to the weights of \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\"), \". The recommendations from \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\"), \" and \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\\u2019\"), \" are then interleaved before being served to users. If the items recommended by \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\\u2019\"), \" get better feedback, the agent updates \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\"), \" towards \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\\u2019\"), \". Otherwise, \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Q\"), \" remains unchanged.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"700px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"95.42857142857143%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe4DaBA0D//EABgQAAIDAAAAAAAAAAAAAAAAAAFBACAx/9oACAEBAAEFAm4dYp//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAUEAEAAAAAAAAAAAAAAAAAAAAw/9oACAEBAAY/Ah//xAAcEAACAgIDAAAAAAAAAAAAAAAAARAhEUExcYH/2gAIAQEAAT8hZ1imnpT6DZXGBpN2JXH/2gAMAwEAAgADAAAAEIMHAP/EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8QH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8QH//EABsQAQADAAMBAAAAAAAAAAAAAAEAESExQWGx/9oACAEBAAE/EFoKGsjZqI3sGyPUQ10yCFW3hc0raqmKaXCH7AAz7P/Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"title\",\n    \"title\": \"title\",\n    \"src\": \"/static/4c563227b30cd3ee2b5a13876a489b10/29d31/ms-dueling-bandit.jpg\",\n    \"srcSet\": [\"/static/4c563227b30cd3ee2b5a13876a489b10/e52aa/ms-dueling-bandit.jpg 175w\", \"/static/4c563227b30cd3ee2b5a13876a489b10/70ebb/ms-dueling-bandit.jpg 350w\", \"/static/4c563227b30cd3ee2b5a13876a489b10/29d31/ms-dueling-bandit.jpg 700w\", \"/static/4c563227b30cd3ee2b5a13876a489b10/4b190/ms-dueling-bandit.jpg 800w\"],\n    \"sizes\": \"(max-width: 700px) 100vw, 700px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(FigureCaption, {\n    caption: \"Exploration via Dueling Bandit Gradient Descent\",\n    source: \"https://dl.acm.org/doi/pdf/10.1145/3178876.3185994\",\n    mdxType: \"FigureCaption\"\n  }), mdx(\"p\", null, \"As a final example, we look at \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ByteDance\\u2019s use of \", mdx(\"a\", {\n    parentName: \"strong\",\n    \"href\": \"https://arxiv.org/abs/2003.00097\"\n  }, \"separate DQNs for recommendations and advertising\"), \".\"), \" They used a two-level framework to jointly optimize recommendations and ads. The first level generates the list of recommendations to optimize for user experience in the long run. The second level inserts ads into the recommendation list to balance between immediate advertising revenue and the negative influence of ads on long-term user experience. \"), mdx(\"p\", null, \"Their recommendation DQN considers the browsing history of items and ads separately. These item and ad sequences go through two GRUs to encode recommendation and ad preferences (similar to JD\\u2019s approach), before being concatenated with the context (e.g., app version, operating system, feed type)\\u2014this represents the state. Potential actions (i.e., item recommendations) are represented via embeddings. Given the state and a list of possible actions, the DQN outputs the Q-value of each state-action pair.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"700px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"48.57142857142858%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAEDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe3NKFH/xAAYEAACAwAAAAAAAAAAAAAAAAABAhEgMf/aAAgBAQABBQI6pmv/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAXEAADAQAAAAAAAAAAAAAAAAAAASBx/9oACAEBAAY/Ah7P/8QAGxABAAICAwAAAAAAAAAAAAAAAQARECExQXH/2gAIAQEAAT8hdDS+EQWXkQb6qBn/2gAMAwEAAgADAAAAEFAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFhEAAwAAAAAAAAAAAAAAAAAAARAx/9oACAECAQE/EBV//8QAGhABAAMBAQEAAAAAAAAAAAAAAQARITEQYf/aAAgBAQABPxCo7nqJ0+xoRRrKwYwb6Qtw7EPP/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"title\",\n    \"title\": \"title\",\n    \"src\": \"/static/6ff71a10817d6eb54673f4adf52f83d8/29d31/bd-rl-recsys.jpg\",\n    \"srcSet\": [\"/static/6ff71a10817d6eb54673f4adf52f83d8/e52aa/bd-rl-recsys.jpg 175w\", \"/static/6ff71a10817d6eb54673f4adf52f83d8/70ebb/bd-rl-recsys.jpg 350w\", \"/static/6ff71a10817d6eb54673f4adf52f83d8/29d31/bd-rl-recsys.jpg 700w\", \"/static/6ff71a10817d6eb54673f4adf52f83d8/4b190/bd-rl-recsys.jpg 800w\"],\n    \"sizes\": \"(max-width: 700px) 100vw, 700px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(FigureCaption, {\n    caption: \"ByteDance's cascading DQN for recommendations\",\n    source: \"https://arxiv.org/abs/2003.00097\",\n    mdxType: \"FigureCaption\"\n  }), mdx(\"p\", null, \"The advertising DQN is tricker. It needs to return three outputs: whether or not to include an ad, which ad to insert, and which position to insert it. However, these outputs are interdependent and a traditional DQN wouldn\\u2019t work. Thus, they designed a novel DQN that outputs the Q-value, specific to an ad, for all possible locations. They also added another unit in the final layer (index=0) to represent the Q-value of \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"not\"), \" inserting an ad. This design lets the DQN consider the state and action and return all three outputs in a single pass.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"700px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"61.142857142857146%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB2aoUD//EABkQAAIDAQAAAAAAAAAAAAAAAAARAhAhQf/aAAgBAQABBQKRwWqv/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhABAQEAAAAAAAAAAAAAAAAAEAEx/9oACAEBAAY/Ana//8QAGxABAAICAwAAAAAAAAAAAAAAAQAREDEhQYH/2gAIAQEAAT8hVLQ63BQc9QY3t4SglY//2gAMAwEAAgADAAAAEIDP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQIBAT8QJ//EAB4QAQEAAgAHAAAAAAAAAAAAAAERACEQMUFRYYHB/9oACAEBAAE/EFHTXga735jslY09dZQrPWHZCIl03IXkNcP/2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"title\",\n    \"title\": \"title\",\n    \"src\": \"/static/b8591b0ffcf2a986288e400dab659dd7/29d31/bd-rl-adsys.jpg\",\n    \"srcSet\": [\"/static/b8591b0ffcf2a986288e400dab659dd7/e52aa/bd-rl-adsys.jpg 175w\", \"/static/b8591b0ffcf2a986288e400dab659dd7/70ebb/bd-rl-adsys.jpg 350w\", \"/static/b8591b0ffcf2a986288e400dab659dd7/29d31/bd-rl-adsys.jpg 700w\", \"/static/b8591b0ffcf2a986288e400dab659dd7/4b190/bd-rl-adsys.jpg 800w\"],\n    \"sizes\": \"(max-width: 700px) 100vw, 700px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(FigureCaption, {\n    caption: \"ByteDance's DQN for advertising\",\n    source: \"https://arxiv.org/abs/2003.00097\",\n    mdxType: \"FigureCaption\"\n  }), mdx(\"p\", null, \"The advertising DQN uses the same method as the recommendation DQN to get the state (via GRUs on item and ad sequences). The output of the recommendation DQN is also included in the state (of the ad DQN). Potential ads are represented via embeddings. \"), mdx(\"p\", null, \"Similar to Microsoft\\u2019s approach, the Q function is separated into value and advantage functions, where the value function doesn\\u2019t consider the ad input. The DQN outputs the Q-value of all ad-location pairs, a proxy for long-term influence of ads on user experience. This is then sent to the bidding system which trades off between immediate ad revenue and long-term Q-values.\"), mdx(\"h2\", {\n    \"id\": \"policy-based-learning-actions-for-each-state-directly\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Policy-based: Learning actions for each state directly\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#policy-based-learning-actions-for-each-state-directly\",\n    \"aria-label\": \"policy based learning actions for each state directly permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"height\": \"20px\",\n    \"width\": \"20px\",\n    \"style\": {\n      \"verticalAlign\": \"-10%\",\n      \"marginLeft\": \"0.25rem\"\n    },\n    \"fill\": \"#007bff\",\n    \"x\": \"0px\",\n    \"y\": \"0px\",\n    \"viewBox\": \"0 0 100 100\",\n    \"enableBackground\": \"new 0 0 100 52\",\n    \"xmlSpace\": \"preserve\"\n  }, mdx(\"g\", {\n    parentName: \"svg\",\n    \"transform\": \"matrix(0.6136733,-0.6136733,0.6136733,0.6136733,4.0598725,65.6509)\"\n  }, mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.807,33.138 c 0.008,-0.009 0.016,-0.019 0.024,-0.026 -0.009,0.007 -0.018,0.015 -0.026,0.023 0,0.001 10e-4,0.002 0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 53.048,49.8 c -0.274,-0.071 -0.545,-0.152 -0.815,-0.232 -0.002,0.001 -0.005,0.003 -0.007,0.003 0.259,0.077 0.534,0.155 0.822,0.229 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 36.33,35.501 h -0.004 c 0.204,0.522 0.496,1.114 0.783,1.651 h 0.013 c -0.284,-0.54 -0.548,-1.09 -0.792,-1.651 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.108,37.152 c 0.291,0.544 0.577,1.029 0.758,1.332 -0.263,-0.436 -0.508,-0.882 -0.746,-1.332 h -0.012 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 38.062,38.804 h 0.006 c -0.048,-0.079 -0.1,-0.152 -0.147,-0.231 0.087,0.144 0.141,0.231 0.141,0.231 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c -0.631,-0.197 -1.249,-0.424 -1.856,-0.669 -0.003,0.002 -0.007,0.003 -0.009,0.004 0.06,0.025 0.781,0.324 1.865,0.665 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 52.226,49.57 c 0.002,0 0.005,-0.002 0.007,-0.003 -0.106,-0.03 -0.213,-0.057 -0.318,-0.091 0.101,0.033 0.205,0.065 0.311,0.094 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 74.458,0.955 H 59.321 c -1.307,0 -2.591,0.1 -3.845,0.293 -0.273,0.042 -0.541,0.101 -0.811,0.151 4.646,2.666 8.469,6.605 10.987,11.343 h 1.238 8.346 c 7.322,0 13.257,5.937 13.257,13.259 0,7.321 -5.936,13.258 -13.257,13.258 H 65.72 63.762 61.857 61.851 58.68 c -3.305,0 -6.324,-1.212 -8.646,-3.212 0,0 -0.003,0.003 -0.003,0.003 0,-0.002 -10e-4,-0.002 -0.002,-0.003 -0.494,-0.414 -0.886,-0.802 -1.206,-1.148 -0.7,-0.764 -1.011,-1.303 -1.011,-1.303 0.001,-0.002 0.002,-0.002 0.002,-0.003 -10e-4,-0.001 -0.002,-0.002 -0.002,-0.003 -1.505,-2.15 -2.389,-4.766 -2.389,-7.589 0,-2.746 0.834,-5.296 2.263,-7.413 0.041,-0.06 0.086,-0.117 0.127,-0.176 -1.736,-1.473 -3.978,-2.367 -6.429,-2.367 h -5.041 c -1.325,3.052 -2.067,6.416 -2.067,9.956 0,3.538 0.737,6.903 2.061,9.955 0.244,0.561 0.509,1.111 0.792,1.651 0.237,0.45 0.482,0.896 0.746,1.332 0.02,0.033 0.036,0.059 0.054,0.088 0.047,0.079 0.099,0.152 0.147,0.231 2.792,4.463 6.948,7.98 11.888,9.973 0.608,0.245 1.226,0.472 1.856,0.669 0.034,0.011 0.068,0.022 0.103,0.031 0.105,0.034 0.212,0.061 0.318,0.091 0.27,0.08 0.541,0.161 0.815,0.232 0.004,0 0.006,0.002 0.01,0.003 2,0.513 4.096,0.787 6.254,0.787 H 74.457 C 88.289,51.044 99.501,39.831 99.501,26 99.502,12.169 88.29,0.955 74.458,0.955 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 25.542,51.045 h 15.137 c 1.308,0 2.592,-0.101 3.846,-0.294 0.273,-0.042 0.542,-0.1 0.811,-0.151 C 40.69,47.935 36.866,43.996 34.349,39.258 H 33.11 24.764 c -7.322,0 -13.257,-5.936 -13.257,-13.259 0,-7.322 5.935,-13.257 13.257,-13.257 h 9.516 1.957 1.905 0.007 3.17 c 3.304,0 6.323,1.212 8.645,3.212 l 0.002,-0.003 c 10e-4,10e-4 10e-4,0.002 0.002,0.003 0.495,0.413 0.887,0.801 1.206,1.148 0.701,0.762 1.011,1.302 1.011,1.302 0,0.001 -0.001,0.002 -0.002,0.003 10e-4,0 0.002,0.001 0.003,0.003 1.505,2.15 2.39,4.766 2.39,7.588 0,2.746 -0.835,5.297 -2.264,7.412 -0.041,0.062 -0.085,0.118 -0.127,0.176 1.736,1.474 3.979,2.369 6.429,2.369 h 5.042 c 1.324,-3.053 2.065,-6.417 2.065,-9.957 0,-3.539 -0.737,-6.903 -2.061,-9.955 C 63.416,15.482 63.151,14.932 62.868,14.392 62.631,13.941 62.386,13.494 62.122,13.06 62.102,13.027 62.086,13 62.069,12.971 62.021,12.893 61.97,12.818 61.922,12.741 59.13,8.278 54.973,4.76 50.034,2.768 49.426,2.523 48.808,2.297 48.178,2.098 48.144,2.087 48.11,2.077 48.075,2.066 47.97,2.034 47.863,2.007 47.757,1.977 47.487,1.896 47.216,1.815 46.941,1.745 46.937,1.744 46.935,1.744 46.932,1.743 44.932,1.228 42.837,0.955 40.677,0.955 H 25.542 c -13.832,0 -25.044,11.212 -25.044,25.044 0,13.833 11.212,25.046 25.044,25.046 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 53.058,49.803 C 53.055,49.802 53.052,49.8 53.048,49.8 c 0.004,0 0.007,0.002 0.01,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c 0.034,0.011 0.068,0.022 0.103,0.031 -0.034,-0.008 -0.068,-0.02 -0.103,-0.031 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.805,33.141 c 0,0 0.311,0.539 1.011,1.303 0.005,-0.006 0.01,-0.012 0.016,-0.015 -0.369,-0.407 -0.709,-0.84 -1.025,-1.291 0,0.001 -0.001,0.001 -0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.92,38.572 c -0.018,-0.029 -0.034,-0.055 -0.054,-0.088 0.019,0.03 0.036,0.058 0.054,0.088 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 48.832,34.429 c -0.006,0.003 -0.011,0.009 -0.016,0.015 0.319,0.347 0.711,0.734 1.206,1.148 0.001,-0.002 0.002,-0.003 0.002,-0.003 -0.42,-0.362 -0.82,-0.749 -1.192,-1.16 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 50.027,35.592 c 10e-4,-0.003 0.004,-0.006 0.012,-0.015 -0.004,0.004 -0.01,0.008 -0.015,0.012 10e-4,0 0.002,0.001 0.003,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"rect\", {\n    parentName: \"g\",\n    \"height\": \"0.004007bff2\",\n    \"width\": \"0.004007bff2\",\n    \"y\": \"35.59\",\n    \"x\": \"50.021999\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }))))), mdx(\"p\", null, \"Relative to value-based methods, policy-based methods learn the policy function that maps the state to action directly, without having to learn Q-values. As a result, they perform better in continuous and stochastic environments (i.e., no discrete states or actions) and tend to be more stable, given a sufficiently small learning rate.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Google shared about \", mdx(\"a\", {\n    parentName: \"strong\",\n    \"href\": \"https://arxiv.org/abs/1812.02353\"\n  }, \"using REINFORCE for YouTube recommendations\"), \".\"), \"  The input is the sequence of user historical interactions, while the output predicts the next action to take (i.e., which video to recommend). \"), mdx(\"p\", null, \"For the main policy (\\u03C0), the input sequence is embedded via an RNN. They tried a variety of RNNs (e.g., LSTMs, GRUs) and ended up using a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1612.06212\"\n  }, \"Chaos Free RNN\"), \" due to its stability and computation efficiency. The RNN takes in the sequence of user events up till time step \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"t\"), \" and encodes it into the user state for the next time step (\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"t+1\"), \"). This is then concatenated with the context of time step \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"t+1\"), \". \"), mdx(\"p\", null, \"The final softmax layer predicts the probability of each action (i.e., videos to recommend)\\u2014this is similar to a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://dl.acm.org/doi/10.1145/2959100.2959190\"\n  }, \"previous implementation\"), \" which also had a softmax output layer. Given that the softmax contains millions of possible actions, they adopt sampled softmax during training. During serving, approximate nearest neighbors (ANN) is applied to retrieve the top actions.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"700px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"74.85714285714286%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIDAQX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHuydSpof/EABoQAAICAwAAAAAAAAAAAAAAAAACAREQITH/2gAIAQEAAQUCbl6gaxYbH//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABkQAAMAAwAAAAAAAAAAAAAAAAABECExQf/aAAgBAQAGPwI3MJHJ/8QAHBAAAgICAwAAAAAAAAAAAAAAAAERMRAhQWFx/9oACAEBAAE/IbRUtl5JS5J9MLsQ4VY//9oADAMBAAIAAwAAABDTz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAECAQE/EIf/xAAdEAEBAAIBBQAAAAAAAAAAAAABEQAhQRAxcbHB/9oACAEBAAE/EGFLvWr8w55HQn1hh2rde+EvIJxmPJSq9P/Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"title\",\n    \"title\": \"title\",\n    \"src\": \"/static/4de84623c5e2ac22b66716572a9975f4/29d31/google-rl.jpg\",\n    \"srcSet\": [\"/static/4de84623c5e2ac22b66716572a9975f4/e52aa/google-rl.jpg 175w\", \"/static/4de84623c5e2ac22b66716572a9975f4/70ebb/google-rl.jpg 350w\", \"/static/4de84623c5e2ac22b66716572a9975f4/29d31/google-rl.jpg 700w\", \"/static/4de84623c5e2ac22b66716572a9975f4/4b190/google-rl.jpg 800w\"],\n    \"sizes\": \"(max-width: 700px) 100vw, 700px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(FigureCaption, {\n    caption: \"Google's REINFORCE main policy with separate behavioral policy head\",\n    source: \"https://arxiv.org/abs/1812.02353\",\n    mdxType: \"FigureCaption\"\n  }), mdx(\"p\", null, \"They also estimate the behavioral policy (\\u03B2). They cite difficulty with directly logging the behavioral policy as there are multiple agents in their system. Thus, for each state-action pair, the behavioral head estimates the probability that the mix of behavioral policies will choose that action using another softmax. From the figure above, they reuse the user-state and model the behavioral policy with a separate head. To prevent the behavioral head from interfering with the user state of the main policy \\u03C0, they block the gradient from flowing back to the RNN.\"), mdx(\"p\", null, \"Boltzmann exploration was used to minimize the negative impact on user experience. They shared that brute force exploration (e.g., e-greedy) was not viable as it could result in inappropriate recommendations and poor user experience. After the top \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"M\"), \" candidates are retrieved via an ANN, the logits (unscaled distances?) are fed into a smaller softmax to normalize the probabilities before sampling from the distribution. Exploration and exploitation are balanced by recommending the top \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"K\"), \" most probable items and sampling the rest from the remaining \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"M - K\"), \" items.\"), mdx(\"h2\", {\n    \"id\": \"actor-critic-combining-value-based-and-policy-based\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Actor-Critic: Combining value-based and policy-based\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#actor-critic-combining-value-based-and-policy-based\",\n    \"aria-label\": \"actor critic combining value based and policy based permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"height\": \"20px\",\n    \"width\": \"20px\",\n    \"style\": {\n      \"verticalAlign\": \"-10%\",\n      \"marginLeft\": \"0.25rem\"\n    },\n    \"fill\": \"#007bff\",\n    \"x\": \"0px\",\n    \"y\": \"0px\",\n    \"viewBox\": \"0 0 100 100\",\n    \"enableBackground\": \"new 0 0 100 52\",\n    \"xmlSpace\": \"preserve\"\n  }, mdx(\"g\", {\n    parentName: \"svg\",\n    \"transform\": \"matrix(0.6136733,-0.6136733,0.6136733,0.6136733,4.0598725,65.6509)\"\n  }, mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.807,33.138 c 0.008,-0.009 0.016,-0.019 0.024,-0.026 -0.009,0.007 -0.018,0.015 -0.026,0.023 0,0.001 10e-4,0.002 0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 53.048,49.8 c -0.274,-0.071 -0.545,-0.152 -0.815,-0.232 -0.002,0.001 -0.005,0.003 -0.007,0.003 0.259,0.077 0.534,0.155 0.822,0.229 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 36.33,35.501 h -0.004 c 0.204,0.522 0.496,1.114 0.783,1.651 h 0.013 c -0.284,-0.54 -0.548,-1.09 -0.792,-1.651 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.108,37.152 c 0.291,0.544 0.577,1.029 0.758,1.332 -0.263,-0.436 -0.508,-0.882 -0.746,-1.332 h -0.012 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 38.062,38.804 h 0.006 c -0.048,-0.079 -0.1,-0.152 -0.147,-0.231 0.087,0.144 0.141,0.231 0.141,0.231 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c -0.631,-0.197 -1.249,-0.424 -1.856,-0.669 -0.003,0.002 -0.007,0.003 -0.009,0.004 0.06,0.025 0.781,0.324 1.865,0.665 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 52.226,49.57 c 0.002,0 0.005,-0.002 0.007,-0.003 -0.106,-0.03 -0.213,-0.057 -0.318,-0.091 0.101,0.033 0.205,0.065 0.311,0.094 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 74.458,0.955 H 59.321 c -1.307,0 -2.591,0.1 -3.845,0.293 -0.273,0.042 -0.541,0.101 -0.811,0.151 4.646,2.666 8.469,6.605 10.987,11.343 h 1.238 8.346 c 7.322,0 13.257,5.937 13.257,13.259 0,7.321 -5.936,13.258 -13.257,13.258 H 65.72 63.762 61.857 61.851 58.68 c -3.305,0 -6.324,-1.212 -8.646,-3.212 0,0 -0.003,0.003 -0.003,0.003 0,-0.002 -10e-4,-0.002 -0.002,-0.003 -0.494,-0.414 -0.886,-0.802 -1.206,-1.148 -0.7,-0.764 -1.011,-1.303 -1.011,-1.303 0.001,-0.002 0.002,-0.002 0.002,-0.003 -10e-4,-0.001 -0.002,-0.002 -0.002,-0.003 -1.505,-2.15 -2.389,-4.766 -2.389,-7.589 0,-2.746 0.834,-5.296 2.263,-7.413 0.041,-0.06 0.086,-0.117 0.127,-0.176 -1.736,-1.473 -3.978,-2.367 -6.429,-2.367 h -5.041 c -1.325,3.052 -2.067,6.416 -2.067,9.956 0,3.538 0.737,6.903 2.061,9.955 0.244,0.561 0.509,1.111 0.792,1.651 0.237,0.45 0.482,0.896 0.746,1.332 0.02,0.033 0.036,0.059 0.054,0.088 0.047,0.079 0.099,0.152 0.147,0.231 2.792,4.463 6.948,7.98 11.888,9.973 0.608,0.245 1.226,0.472 1.856,0.669 0.034,0.011 0.068,0.022 0.103,0.031 0.105,0.034 0.212,0.061 0.318,0.091 0.27,0.08 0.541,0.161 0.815,0.232 0.004,0 0.006,0.002 0.01,0.003 2,0.513 4.096,0.787 6.254,0.787 H 74.457 C 88.289,51.044 99.501,39.831 99.501,26 99.502,12.169 88.29,0.955 74.458,0.955 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 25.542,51.045 h 15.137 c 1.308,0 2.592,-0.101 3.846,-0.294 0.273,-0.042 0.542,-0.1 0.811,-0.151 C 40.69,47.935 36.866,43.996 34.349,39.258 H 33.11 24.764 c -7.322,0 -13.257,-5.936 -13.257,-13.259 0,-7.322 5.935,-13.257 13.257,-13.257 h 9.516 1.957 1.905 0.007 3.17 c 3.304,0 6.323,1.212 8.645,3.212 l 0.002,-0.003 c 10e-4,10e-4 10e-4,0.002 0.002,0.003 0.495,0.413 0.887,0.801 1.206,1.148 0.701,0.762 1.011,1.302 1.011,1.302 0,0.001 -0.001,0.002 -0.002,0.003 10e-4,0 0.002,0.001 0.003,0.003 1.505,2.15 2.39,4.766 2.39,7.588 0,2.746 -0.835,5.297 -2.264,7.412 -0.041,0.062 -0.085,0.118 -0.127,0.176 1.736,1.474 3.979,2.369 6.429,2.369 h 5.042 c 1.324,-3.053 2.065,-6.417 2.065,-9.957 0,-3.539 -0.737,-6.903 -2.061,-9.955 C 63.416,15.482 63.151,14.932 62.868,14.392 62.631,13.941 62.386,13.494 62.122,13.06 62.102,13.027 62.086,13 62.069,12.971 62.021,12.893 61.97,12.818 61.922,12.741 59.13,8.278 54.973,4.76 50.034,2.768 49.426,2.523 48.808,2.297 48.178,2.098 48.144,2.087 48.11,2.077 48.075,2.066 47.97,2.034 47.863,2.007 47.757,1.977 47.487,1.896 47.216,1.815 46.941,1.745 46.937,1.744 46.935,1.744 46.932,1.743 44.932,1.228 42.837,0.955 40.677,0.955 H 25.542 c -13.832,0 -25.044,11.212 -25.044,25.044 0,13.833 11.212,25.046 25.044,25.046 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 53.058,49.803 C 53.055,49.802 53.052,49.8 53.048,49.8 c 0.004,0 0.007,0.002 0.01,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c 0.034,0.011 0.068,0.022 0.103,0.031 -0.034,-0.008 -0.068,-0.02 -0.103,-0.031 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.805,33.141 c 0,0 0.311,0.539 1.011,1.303 0.005,-0.006 0.01,-0.012 0.016,-0.015 -0.369,-0.407 -0.709,-0.84 -1.025,-1.291 0,0.001 -0.001,0.001 -0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.92,38.572 c -0.018,-0.029 -0.034,-0.055 -0.054,-0.088 0.019,0.03 0.036,0.058 0.054,0.088 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 48.832,34.429 c -0.006,0.003 -0.011,0.009 -0.016,0.015 0.319,0.347 0.711,0.734 1.206,1.148 0.001,-0.002 0.002,-0.003 0.002,-0.003 -0.42,-0.362 -0.82,-0.749 -1.192,-1.16 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 50.027,35.592 c 10e-4,-0.003 0.004,-0.006 0.012,-0.015 -0.004,0.004 -0.01,0.008 -0.015,0.012 10e-4,0 0.002,0.001 0.003,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"rect\", {\n    parentName: \"g\",\n    \"height\": \"0.004007bff2\",\n    \"width\": \"0.004007bff2\",\n    \"y\": \"35.59\",\n    \"x\": \"50.021999\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }))))), mdx(\"p\", null, \"Actor-critic combines the best of value-based and policy-based methods by splitting the model into two, one for computing the action based on state, and another to produce the Q-value of the state-action. The (policy-based) actor takes state as input and outputs the best action, learning the optimal policy and controlling how the agent behaves. The (value-based) critic then evaluates the action by computing the value function and providing feedback, to the actor, on how good the action is.\"), mdx(\"p\", null, \"I came across several examples of actor-critic used in recommendations and search. We won\\u2019t be discussing them in this piece and I recommend you read them as required.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Alibaba\\u2019s use of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.00710\"\n  }, \"actor-critic to choose the best ranking function\"), \" for a search query\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Baidu\\u2019s use of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1902.03987\"\n  }, \"two actors, one for home page recs and one for detail page recs\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"JD\\u2019s use of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.02343\"\n  }, \"actor-critic for page-wise recommendations\"))), mdx(\"h2\", {\n    \"id\": \"conclusion\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Conclusion\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#conclusion\",\n    \"aria-label\": \"conclusion permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"height\": \"20px\",\n    \"width\": \"20px\",\n    \"style\": {\n      \"verticalAlign\": \"-10%\",\n      \"marginLeft\": \"0.25rem\"\n    },\n    \"fill\": \"#007bff\",\n    \"x\": \"0px\",\n    \"y\": \"0px\",\n    \"viewBox\": \"0 0 100 100\",\n    \"enableBackground\": \"new 0 0 100 52\",\n    \"xmlSpace\": \"preserve\"\n  }, mdx(\"g\", {\n    parentName: \"svg\",\n    \"transform\": \"matrix(0.6136733,-0.6136733,0.6136733,0.6136733,4.0598725,65.6509)\"\n  }, mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.807,33.138 c 0.008,-0.009 0.016,-0.019 0.024,-0.026 -0.009,0.007 -0.018,0.015 -0.026,0.023 0,0.001 10e-4,0.002 0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 53.048,49.8 c -0.274,-0.071 -0.545,-0.152 -0.815,-0.232 -0.002,0.001 -0.005,0.003 -0.007,0.003 0.259,0.077 0.534,0.155 0.822,0.229 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 36.33,35.501 h -0.004 c 0.204,0.522 0.496,1.114 0.783,1.651 h 0.013 c -0.284,-0.54 -0.548,-1.09 -0.792,-1.651 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.108,37.152 c 0.291,0.544 0.577,1.029 0.758,1.332 -0.263,-0.436 -0.508,-0.882 -0.746,-1.332 h -0.012 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 38.062,38.804 h 0.006 c -0.048,-0.079 -0.1,-0.152 -0.147,-0.231 0.087,0.144 0.141,0.231 0.141,0.231 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c -0.631,-0.197 -1.249,-0.424 -1.856,-0.669 -0.003,0.002 -0.007,0.003 -0.009,0.004 0.06,0.025 0.781,0.324 1.865,0.665 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 52.226,49.57 c 0.002,0 0.005,-0.002 0.007,-0.003 -0.106,-0.03 -0.213,-0.057 -0.318,-0.091 0.101,0.033 0.205,0.065 0.311,0.094 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 74.458,0.955 H 59.321 c -1.307,0 -2.591,0.1 -3.845,0.293 -0.273,0.042 -0.541,0.101 -0.811,0.151 4.646,2.666 8.469,6.605 10.987,11.343 h 1.238 8.346 c 7.322,0 13.257,5.937 13.257,13.259 0,7.321 -5.936,13.258 -13.257,13.258 H 65.72 63.762 61.857 61.851 58.68 c -3.305,0 -6.324,-1.212 -8.646,-3.212 0,0 -0.003,0.003 -0.003,0.003 0,-0.002 -10e-4,-0.002 -0.002,-0.003 -0.494,-0.414 -0.886,-0.802 -1.206,-1.148 -0.7,-0.764 -1.011,-1.303 -1.011,-1.303 0.001,-0.002 0.002,-0.002 0.002,-0.003 -10e-4,-0.001 -0.002,-0.002 -0.002,-0.003 -1.505,-2.15 -2.389,-4.766 -2.389,-7.589 0,-2.746 0.834,-5.296 2.263,-7.413 0.041,-0.06 0.086,-0.117 0.127,-0.176 -1.736,-1.473 -3.978,-2.367 -6.429,-2.367 h -5.041 c -1.325,3.052 -2.067,6.416 -2.067,9.956 0,3.538 0.737,6.903 2.061,9.955 0.244,0.561 0.509,1.111 0.792,1.651 0.237,0.45 0.482,0.896 0.746,1.332 0.02,0.033 0.036,0.059 0.054,0.088 0.047,0.079 0.099,0.152 0.147,0.231 2.792,4.463 6.948,7.98 11.888,9.973 0.608,0.245 1.226,0.472 1.856,0.669 0.034,0.011 0.068,0.022 0.103,0.031 0.105,0.034 0.212,0.061 0.318,0.091 0.27,0.08 0.541,0.161 0.815,0.232 0.004,0 0.006,0.002 0.01,0.003 2,0.513 4.096,0.787 6.254,0.787 H 74.457 C 88.289,51.044 99.501,39.831 99.501,26 99.502,12.169 88.29,0.955 74.458,0.955 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 25.542,51.045 h 15.137 c 1.308,0 2.592,-0.101 3.846,-0.294 0.273,-0.042 0.542,-0.1 0.811,-0.151 C 40.69,47.935 36.866,43.996 34.349,39.258 H 33.11 24.764 c -7.322,0 -13.257,-5.936 -13.257,-13.259 0,-7.322 5.935,-13.257 13.257,-13.257 h 9.516 1.957 1.905 0.007 3.17 c 3.304,0 6.323,1.212 8.645,3.212 l 0.002,-0.003 c 10e-4,10e-4 10e-4,0.002 0.002,0.003 0.495,0.413 0.887,0.801 1.206,1.148 0.701,0.762 1.011,1.302 1.011,1.302 0,0.001 -0.001,0.002 -0.002,0.003 10e-4,0 0.002,0.001 0.003,0.003 1.505,2.15 2.39,4.766 2.39,7.588 0,2.746 -0.835,5.297 -2.264,7.412 -0.041,0.062 -0.085,0.118 -0.127,0.176 1.736,1.474 3.979,2.369 6.429,2.369 h 5.042 c 1.324,-3.053 2.065,-6.417 2.065,-9.957 0,-3.539 -0.737,-6.903 -2.061,-9.955 C 63.416,15.482 63.151,14.932 62.868,14.392 62.631,13.941 62.386,13.494 62.122,13.06 62.102,13.027 62.086,13 62.069,12.971 62.021,12.893 61.97,12.818 61.922,12.741 59.13,8.278 54.973,4.76 50.034,2.768 49.426,2.523 48.808,2.297 48.178,2.098 48.144,2.087 48.11,2.077 48.075,2.066 47.97,2.034 47.863,2.007 47.757,1.977 47.487,1.896 47.216,1.815 46.941,1.745 46.937,1.744 46.935,1.744 46.932,1.743 44.932,1.228 42.837,0.955 40.677,0.955 H 25.542 c -13.832,0 -25.044,11.212 -25.044,25.044 0,13.833 11.212,25.046 25.044,25.046 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 53.058,49.803 C 53.055,49.802 53.052,49.8 53.048,49.8 c 0.004,0 0.007,0.002 0.01,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c 0.034,0.011 0.068,0.022 0.103,0.031 -0.034,-0.008 -0.068,-0.02 -0.103,-0.031 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.805,33.141 c 0,0 0.311,0.539 1.011,1.303 0.005,-0.006 0.01,-0.012 0.016,-0.015 -0.369,-0.407 -0.709,-0.84 -1.025,-1.291 0,0.001 -0.001,0.001 -0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.92,38.572 c -0.018,-0.029 -0.034,-0.055 -0.054,-0.088 0.019,0.03 0.036,0.058 0.054,0.088 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 48.832,34.429 c -0.006,0.003 -0.011,0.009 -0.016,0.015 0.319,0.347 0.711,0.734 1.206,1.148 0.001,-0.002 0.002,-0.003 0.002,-0.003 -0.42,-0.362 -0.82,-0.749 -1.192,-1.16 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 50.027,35.592 c 10e-4,-0.003 0.004,-0.006 0.012,-0.015 -0.004,0.004 -0.01,0.008 -0.015,0.012 10e-4,0 0.002,0.001 0.003,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"rect\", {\n    parentName: \"g\",\n    \"height\": \"0.004007bff2\",\n    \"width\": \"0.004007bff2\",\n    \"y\": \"35.59\",\n    \"x\": \"50.021999\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }))))), mdx(\"p\", null, \"Are you trying to apply reinforcement learning for recommendation problems? If so, contextual bandits might be a good place to start\\u2014they\\u2019re relatively simpler to implement and might not need as much data as deep learning approaches. Beyond contextual bandits, DQNs seem to be the go-to baseline.\"), mdx(\"p\", null, \"Nonetheless, regardless of which approach you adopt, pay extra attention to how off-policy evaluation is done\\u2014having the right evaluation framework is more than half the battle won. Here\\u2019s a good \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.cs.cornell.edu/~adith/CfactSIGIR2016/\"\n  }, \"tutorial\"), \" on counterfactual evaluation. Also, see this \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2104.08912\"\n  }, \"paper\"), \" on Simpson\\u2019s Paradox in offline evaluation for recommendations.\"), mdx(\"h2\", {\n    \"id\": \"references\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"References\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#references\",\n    \"aria-label\": \"references permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"height\": \"20px\",\n    \"width\": \"20px\",\n    \"style\": {\n      \"verticalAlign\": \"-10%\",\n      \"marginLeft\": \"0.25rem\"\n    },\n    \"fill\": \"#007bff\",\n    \"x\": \"0px\",\n    \"y\": \"0px\",\n    \"viewBox\": \"0 0 100 100\",\n    \"enableBackground\": \"new 0 0 100 52\",\n    \"xmlSpace\": \"preserve\"\n  }, mdx(\"g\", {\n    parentName: \"svg\",\n    \"transform\": \"matrix(0.6136733,-0.6136733,0.6136733,0.6136733,4.0598725,65.6509)\"\n  }, mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.807,33.138 c 0.008,-0.009 0.016,-0.019 0.024,-0.026 -0.009,0.007 -0.018,0.015 -0.026,0.023 0,0.001 10e-4,0.002 0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 53.048,49.8 c -0.274,-0.071 -0.545,-0.152 -0.815,-0.232 -0.002,0.001 -0.005,0.003 -0.007,0.003 0.259,0.077 0.534,0.155 0.822,0.229 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 36.33,35.501 h -0.004 c 0.204,0.522 0.496,1.114 0.783,1.651 h 0.013 c -0.284,-0.54 -0.548,-1.09 -0.792,-1.651 z\",\n    \"style\": {},\n    \"fill\": \"none\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.108,37.152 c 0.291,0.544 0.577,1.029 0.758,1.332 -0.263,-0.436 -0.508,-0.882 -0.746,-1.332 h -0.012 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 38.062,38.804 h 0.006 c -0.048,-0.079 -0.1,-0.152 -0.147,-0.231 0.087,0.144 0.141,0.231 0.141,0.231 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c -0.631,-0.197 -1.249,-0.424 -1.856,-0.669 -0.003,0.002 -0.007,0.003 -0.009,0.004 0.06,0.025 0.781,0.324 1.865,0.665 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 52.226,49.57 c 0.002,0 0.005,-0.002 0.007,-0.003 -0.106,-0.03 -0.213,-0.057 -0.318,-0.091 0.101,0.033 0.205,0.065 0.311,0.094 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 74.458,0.955 H 59.321 c -1.307,0 -2.591,0.1 -3.845,0.293 -0.273,0.042 -0.541,0.101 -0.811,0.151 4.646,2.666 8.469,6.605 10.987,11.343 h 1.238 8.346 c 7.322,0 13.257,5.937 13.257,13.259 0,7.321 -5.936,13.258 -13.257,13.258 H 65.72 63.762 61.857 61.851 58.68 c -3.305,0 -6.324,-1.212 -8.646,-3.212 0,0 -0.003,0.003 -0.003,0.003 0,-0.002 -10e-4,-0.002 -0.002,-0.003 -0.494,-0.414 -0.886,-0.802 -1.206,-1.148 -0.7,-0.764 -1.011,-1.303 -1.011,-1.303 0.001,-0.002 0.002,-0.002 0.002,-0.003 -10e-4,-0.001 -0.002,-0.002 -0.002,-0.003 -1.505,-2.15 -2.389,-4.766 -2.389,-7.589 0,-2.746 0.834,-5.296 2.263,-7.413 0.041,-0.06 0.086,-0.117 0.127,-0.176 -1.736,-1.473 -3.978,-2.367 -6.429,-2.367 h -5.041 c -1.325,3.052 -2.067,6.416 -2.067,9.956 0,3.538 0.737,6.903 2.061,9.955 0.244,0.561 0.509,1.111 0.792,1.651 0.237,0.45 0.482,0.896 0.746,1.332 0.02,0.033 0.036,0.059 0.054,0.088 0.047,0.079 0.099,0.152 0.147,0.231 2.792,4.463 6.948,7.98 11.888,9.973 0.608,0.245 1.226,0.472 1.856,0.669 0.034,0.011 0.068,0.022 0.103,0.031 0.105,0.034 0.212,0.061 0.318,0.091 0.27,0.08 0.541,0.161 0.815,0.232 0.004,0 0.006,0.002 0.01,0.003 2,0.513 4.096,0.787 6.254,0.787 H 74.457 C 88.289,51.044 99.501,39.831 99.501,26 99.502,12.169 88.29,0.955 74.458,0.955 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 25.542,51.045 h 15.137 c 1.308,0 2.592,-0.101 3.846,-0.294 0.273,-0.042 0.542,-0.1 0.811,-0.151 C 40.69,47.935 36.866,43.996 34.349,39.258 H 33.11 24.764 c -7.322,0 -13.257,-5.936 -13.257,-13.259 0,-7.322 5.935,-13.257 13.257,-13.257 h 9.516 1.957 1.905 0.007 3.17 c 3.304,0 6.323,1.212 8.645,3.212 l 0.002,-0.003 c 10e-4,10e-4 10e-4,0.002 0.002,0.003 0.495,0.413 0.887,0.801 1.206,1.148 0.701,0.762 1.011,1.302 1.011,1.302 0,0.001 -0.001,0.002 -0.002,0.003 10e-4,0 0.002,0.001 0.003,0.003 1.505,2.15 2.39,4.766 2.39,7.588 0,2.746 -0.835,5.297 -2.264,7.412 -0.041,0.062 -0.085,0.118 -0.127,0.176 1.736,1.474 3.979,2.369 6.429,2.369 h 5.042 c 1.324,-3.053 2.065,-6.417 2.065,-9.957 0,-3.539 -0.737,-6.903 -2.061,-9.955 C 63.416,15.482 63.151,14.932 62.868,14.392 62.631,13.941 62.386,13.494 62.122,13.06 62.102,13.027 62.086,13 62.069,12.971 62.021,12.893 61.97,12.818 61.922,12.741 59.13,8.278 54.973,4.76 50.034,2.768 49.426,2.523 48.808,2.297 48.178,2.098 48.144,2.087 48.11,2.077 48.075,2.066 47.97,2.034 47.863,2.007 47.757,1.977 47.487,1.896 47.216,1.815 46.941,1.745 46.937,1.744 46.935,1.744 46.932,1.743 44.932,1.228 42.837,0.955 40.677,0.955 H 25.542 c -13.832,0 -25.044,11.212 -25.044,25.044 0,13.833 11.212,25.046 25.044,25.046 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"M 53.058,49.803 C 53.055,49.802 53.052,49.8 53.048,49.8 c 0.004,0 0.007,0.002 0.01,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 51.812,49.445 c 0.034,0.011 0.068,0.022 0.103,0.031 -0.034,-0.008 -0.068,-0.02 -0.103,-0.031 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 47.805,33.141 c 0,0 0.311,0.539 1.011,1.303 0.005,-0.006 0.01,-0.012 0.016,-0.015 -0.369,-0.407 -0.709,-0.84 -1.025,-1.291 0,0.001 -0.001,0.001 -0.002,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 37.92,38.572 c -0.018,-0.029 -0.034,-0.055 -0.054,-0.088 0.019,0.03 0.036,0.058 0.054,0.088 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 48.832,34.429 c -0.006,0.003 -0.011,0.009 -0.016,0.015 0.319,0.347 0.711,0.734 1.206,1.148 0.001,-0.002 0.002,-0.003 0.002,-0.003 -0.42,-0.362 -0.82,-0.749 -1.192,-1.16 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"path\", {\n    parentName: \"g\",\n    \"d\": \"m 50.027,35.592 c 10e-4,-0.003 0.004,-0.006 0.012,-0.015 -0.004,0.004 -0.01,0.008 -0.015,0.012 10e-4,0 0.002,0.001 0.003,0.003 z\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }), mdx(\"rect\", {\n    parentName: \"g\",\n    \"height\": \"0.004007bff2\",\n    \"width\": \"0.004007bff2\",\n    \"y\": \"35.59\",\n    \"x\": \"50.021999\",\n    \"style\": {},\n    \"fill\": \"#007bff\"\n  }))))), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1003.0146\"\n  }, \"A Contextual-Bandit Approach to Personalized News Article Recommendation\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://netflixtechblog.com/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6\"\n  }, \"Selecting the best artwork for videos through A/B testing\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://netflixtechblog.com/artwork-personalization-c589f074ad76\"\n  }, \"Artwork Personalization at Netflix\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.06501\"\n  }, \"Recommendations with Negative Feedback via Deep Reinforcement Learning\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://dl.acm.org/doi/pdf/10.1145/3178876.3185994\"\n  }, \"DRN: A Deep Reinforcement Learning Framework for News Recommendation\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2003.00097\"\n  }, \"Jointly Learning to Recommend and Advertise\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.02353\"\n  }, \"Top-K Off-Policy Correction for a REINFORCE Recommender System\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.06212\"\n  }, \"A recurrent neural network without chaos\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://dl.acm.org/doi/10.1145/2959100.2959190\"\n  }, \"Deep Neural Networks for YouTube Recommendations\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.00710\"\n  }, \"Reinforcement Learning to Rank in E-Commerce Search Engine\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1902.03987\"\n  }, \"Whole-Chain Recommendations\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.02343\"\n  }, \"Deep Reinforcement Learning for Page-wise Recommendations\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.cornell.edu/~adith/CfactSIGIR2016/\"\n  }, \"SIGIR 2016 Tutorial on Counterfactual Evaluation and Learning\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2104.08912\"\n  }, \"The Simpson's Paradox in the Offline Evaluation of Recommendation Systems\"))));\n}\n;\nMDXContent.isMDXComponent = true;"}},"pageContext":{"slug":"/resources/rl-for-recsys/"}},
    "staticQueryHashes": ["3159585216","3897982121"]}