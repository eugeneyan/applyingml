---
name: "Pranam Janney"
role: "Principal Data Scientist @ Scentre Group"
description: ""
date: "2022-02-20"
slug: "/mentors/pranam-janney/"
type: "mentor"
---
Learn more about Pranam Janney on his [LinkedIn](https://www.linkedin.com/in/pranamjanney/).

### *Please share a bit about yourself: your current role, where you work, and what you do?*
I currently work as a Principal Data Scientist at [Scentre Group](https://www.scentregroup.com/home-page) within Strategic Analytics, Insights & Research team. Scentre Group owns and manages [Westfield](https://www.westfield.com.au/) shopping centres in Australia and New Zealand, with a sole purpose of *Creating extraordinary places, connecting and enriching communities*. Our team focuses on enabling our Westfields to better serve our customers and retailers. 


### *What was your path towards working with machine learning? What factors helped along the way?*
To be honest, I never set out to become a data scientist or a machine learning practitioner.  I was fortunate enough to have circumstances that allowed me to do what I wanted and I did. I took up telecommunications engineering in my bachelors and picked up digital signal processing. The concepts within signal processing piqued my interest, which led to me coding a lot on Matlab. Back then Matlab was the preferred tool of choice and one could do a lot using it. That led me to a Master's degree in electrical engineering, where I realised one dimensional signal processing could lead to two-dimensional signal processing aka image processing. So, my Master's project was in image processing. I then added another dimension to image processing i.e. time dimension, and applied for PhD at [The University of New South Wales](https://www.unsw.edu.au/) and [National ICT Australia](https://en.wikipedia.org/wiki/NICTA) with a proposal that was around video understanding/summarisation. All throughout, I was using data, advanced statistics/calculus and machine learning algorithms as tools to achieve my goal. When I look back and think about it, the factors that actually helped a lot was my eagerness and curiosity to solve problems. 


### *How do you spend your time day-to-day?*
My time is now split between exploring new greenfield projects, developing & productionising analytics products and managing our deliverables & production environment. 


### *How do you work with business to identify and define problems suited for machine learning? How do you align ML projects with business objectives?*
Machine learning, in my opinion, is a tool to solve a problem. We do not seek out problems for machine learning specifically. We operate very much like a consulting firm e.g. Bain & Company or McKinsey. When we speak to the business or a stakeholder, we try to understand their goals or painpoints and come up with a set of deliverables that could enable them to achieve their goals or alleviates their painpoints using a top-down approach. Bottom most layer is the raw data layer and the top most layer is the deliverable.  All this happens consultatively and conceptually and most likely we will have not touched any data or analytics by then. Of course, this needs a thorough understanding of our existing data, algorithms and their limits and capabilities. In classical academic research, this would be called a *proposal*, we then come up with a plan to execute this proposal. Sometimes, our proposal might become infeasible due to limits of the data/algorithms/etc that was previously unknown to us. We then create assumptions (thereby approximating the solution) or find proxy data that could fill the gap. Every solution to a business problem can be enabled by data science but every data science solution may not necessarily solve a business problem. 

### *Imagine you're given a new, unfamiliar problem to solve with machine learning. How would you approach it?*
In my field of work, the stakeholders have problems that need simplification or a solution. Taking a step back; machine learning is a sub-field of data science. Keeping that in context, if I am asked the same question by replacing 'machine learning' with 'data science' then usually I follow the below steps in that order:
- What is the stakeholder trying to achieve? (i.e. As a sales executive, I want to...)
- What inputs could the stakeholder use to achieve their goal (i.e. As a data scientist, I have deduced that a sales executive will need ...)
- Inputs to stakeholder is our outputs. Solution to which problem statement could provide these outputs? (i.e. As a data scientist, I will solve...)
- What data and/or data-concepts do I need to find a solution to this problem statement? (i.e. As a data scientist, I will need to derive/use...)
- Do we have data that is proxy or able to derive the proxy for these data concepts?
- How much data do we have?
- Is it clean?
- Is it representative of the target/proxy?
- What is the simplest solution (e.g. using statistics/heuristics)? - Does it get me to [80/20 output](https://www.investopedia.com/terms/1/80-20-rule.asp)?
- How do I improve upon this output? - What additional data concepts would be useful from here on? 
- Go to step-1 

This is a classical top-down approach, starting with the highest level user-story and distilling layer by layer from the top. I believe, this is applicable in every aspect of data science.

### *Designing, building, and operating ML systems is a big effort. Who do you collaborate with? How do you scale yourself?*
We have data engineering capability within the team. However, there is a separate data engineering team within the wider organisation with whom we collaborate for integrating complex ML systems with datawarehouse and other legacy systems and to scale up.

### *There are many ways to structure DS/ML teams—what have you seen work, or not work?*
This is a very difficult question and the generic answer is "it depends". Data science teams are in my view, mini-startups in their own right.

From the organisation's perspective, when building a data science team, one must take into consideration necessary functions that the team will need or be skilled in, such as science/mathematics, behavourial science, machine learning, data engineering & devops, stakeholder management, business knowledge/analysis, visualisation/user experience, etc. A big organisation that is introducing data science into their capability matrix, most likely will have most of these functions already, excluding science/mathematics, behavourial science and machine learning, spread across different business units. So, more likely than not they will embark on building a data science team with only scientists and machine learning engineers and call upon the pre-existing functions as and when required from other business units. However, I have not yet seen an instance of data science team built with this thinking be successful. Especially in a big and/or legacy organisation, it is quite important to seed and build a data science team as a start-up that encompasses all the required functions within itself until such point where data science is proven and widely used capability within the organisation. Once, data science capability reaches a certain maturity, going into a hub-spoke model  or a centre-of-excellence model would be ideal depending on the needs of the organisation.

From an employee/individual perspective, an all-rounder data scientist is much more valuable and has superior career prospects. What I mean by all-rounder data scientist is a data scientist who is skilled in all aspects of data science and various business domains/fields. Therefore, rotation of data scientists across all necessary aspects of a data science business function and also rotation across all business domains within the organisation should be an important charter for a data science team. This not only inspires data scientists to join your team but also is a strong motivator for data scientist to remain in the team/organisation. Implementing this charter will not be a problem in the start-up phase of the team but will become difficult when the team progresses into hub-spoke or centre-of-excellence model. It is the responsbility of the Director [^1] and/or Chief/Principal Data Scientist to ensure rotation of data scientists across functions and domains by lobbying the cause  with business stakeholders.

Rotation is also helpful from business-continuity perspective as you will eliminate the single-person-risk in any data science function. Good data scientists are scarce therefore single-person-risk has an exponential impact on data science functions when compared to other business functions. 

### *How do you quantify the impact of your work? What was the greatest impact you made?*

During early part of my career, I happened to walk out of the office at the same time as my then CFO to pick up lunch. I started small talk and in my efforts to sound smart, I mentioned to him that it was difficult to measure impact of data science. His answer was quite blunt. He said "find the end metric your stakeholders are trying to improve, you have your measurement right there". I stopped trying to sound smart after that.

I think [KISS principle](https://en.wikipedia.org/wiki/KISS_principle) is applicable here. Aligning your goals to the goals of the stakeholder is the easiest way to quantify impacts of one's work. Usually, the stakeholder are one of three types, they:
- have an issue that needs investigation/solution  OR
- need an improvement OR
- need a greenfield product that solves X.

Irrespective of which bucket they fall into, they have an end metric that they are all trying to improve upon (e.g. sales, volume, reduce costs, etc). For example, the above three buckets would have end metrics such as,
- Issue is creating X amount of loss (due to inefficiency, errors). 
- Currently doing X throughput, we need to improve upon this.
- We have deficiency in area Y, we need to achieve X in area Y.

As part of your role as a data scientist, you should understand the end metric, manage expectations for measurements and deliver against that. Therefore, measuring impact of one's work is a product of good understanding of your stakeholder's expectations and well thought out problem statement. 

One has to be very mindful that a data scientist's job does not finish after hand-over of a product. You have to do health-checks with the business stakeholders periodically. This enables you to better understand and chart impact of your work. 

In terms of the greatest impacts, I can only list my failures (failure might be a strong negative phrase) because that is what I track personally. It might sound counter intuitive. In research and in data science, keep track of failures and things that does not help/work, because that gets you to your next goal faster. If you are working towards a solution then that solution, by definition, will have significant impact on the business otherwise you would/should not be working on it.

### *After shipping your ML project, how do you monitor performance in production? Did you have to update pipelines or retrain models—how manual or automatic was this?*
Monitoring performance is one of the key pillars of a ML DevOps framework. ML pipeline cannot be considered to be in-production if it does not have a performance monitoring framework around it. There are a few concepts here, elaborating on them might shed more light.  
* ML Pipeline - an approximate version of how a human would make decision at a point in time, based on the various data points that are available from various other systems.
* In-production - would mean the pipeline is running on a schedule, feeding various target systems thereby providing users live actionable-insights.

We live in a World that is made up of random variables and many random variables are explicitly or implicitly related to one another thus making many of them [non-stationary](https://www.investopedia.com/articles/trading/07/stationary.asp). A variable deemed stationary at a point in time, might turn out to be non-stationary at a future point in time and vice versa. ML pipelines are built using these variables that exist in this World as is or directly/indirectly correlated to a variable that exists in this World. Through passage of time, variables tend to deviate, thereby the machine-learnt model tends to deviate away. You should not be serving deviated insights to users from in-production models therefore a constant performance monitoring regime is warranted. By constantly measuring performance of the ML pipeline, one can identify when the model is drifting/deviating and take appropriate actions. 

If the model is drifting, the appropriate action could depend upon the diagnosis. It could be as straightforward as retraining using updated data or as involved as starting from scratch i.e. rethinking the problem itself. 

### *Think of people who are able to apply ML effectively–what skills or traits do you think contributed to that?*
In my career, I have noticed that most people who are effective data scientist and/or ML practioners display the below qualities, in no particular order, 
* enterprenurial
* builders - innate urge to build/create things
* problem solvers - inspired to solve problems big or small
* curiosity - innate urge to understand/learn about anything and everything
* communication skills - ability to summarise/distill information and communicate succintly, ability to define a problem or present a solution without using too many technical jargon.

Pick a problem and try to solve it, along the way you will pick up the tools that are required to solve the problem. Sounds very simple, but I have seen lots of people chase the tools and forget the concept of solving a problem.  

Therefore, when we are hiring we look for the above qualities.

### *Do you have any lessons or advice about applying ML that's especially helpful? Anything that you didn't learn at school or via a book (i.e., only at work)?*
When I was doing my Master's and in  my PhD, the main goal was to build solutions that 'do better' than existing state of the art. For example, the existing state of the art produces 90% accuracy then the goal was to beat the 90% accuracy (*depth*). I was honed in on metrics such as accuracy, performance, faster, quicker, better, etc. As I moved into the commercial setting, I found myself working on defining appropriate problem statments, building solutions to problems using [80-20 rule](https://www.investopedia.com/terms/1/80-20-rule.asp), stakeholder management and relationship management (*breadth*). 

Schools are great places to get into the *depth* aspect of a subject/domain. You can do hundreds of courses/projects related to *breadth* but you will most likely be viewed as second to someone who has achieved *breadth* in a commercial setting. On the other hand, commercial environments are great places to get into *breadth* and they rarely offer opportunities to get into *depth*. So, my advice to anyone is to choose wisely, depending on what you want to achieve in your professional life. 

As I have noted earlier, we look for builders & problem-solvers. If someone builds solution to problems in their free time, that is the person everybody wants. Think about how you can showcase your enterprenurial aspect when applying for jobs or roles. 

### *How do you learn continuously? What are some resources or role models that you've learned from?*
Your website/newsletter is a great source of information to keep me updated :-). MOOC's such as Courseera are a great source for learning new things. Also, I read a variety of blogs ranging from mathematics / algorithms to science & technology to policy/regulatory to econometrics. As a data scientist, I believe it is quite important to be well rounded in understanding how these fields are progressing and how they interact and influence each other.

[^1] Australian based company title hierarchy is a bit different to the US. A Director in an Australian based company would be similar to EVP/SVP in the US, I believe. 